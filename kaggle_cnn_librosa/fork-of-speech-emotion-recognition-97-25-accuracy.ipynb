{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555},{"sourceId":6060815,"sourceType":"datasetVersion","datasetId":3468263},{"sourceId":10547799,"sourceType":"datasetVersion","datasetId":6526209},{"sourceId":242456,"sourceType":"modelInstanceVersion","modelInstanceId":207086,"modelId":228821}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n#IMPORT THE LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nimport IPython.display as ipd\nfrom IPython.display import Audio\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization , GRU\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD\n\n\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nimport tensorflow as tf \nprint (\"Done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:13.220177Z","iopub.execute_input":"2025-05-11T14:08:13.220711Z","iopub.status.idle":"2025-05-11T14:08:14.177235Z","shell.execute_reply.started":"2025-05-11T14:08:13.220685Z","shell.execute_reply":"2025-05-11T14:08:14.176634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get update\n!apt-get install -y libsndfile1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:21.311767Z","iopub.execute_input":"2025-05-11T14:08:21.312412Z","iopub.status.idle":"2025-05-11T14:08:27.917277Z","shell.execute_reply.started":"2025-05-11T14:08:21.312390Z","shell.execute_reply":"2025-05-11T14:08:27.916549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing Data ","metadata":{}},{"cell_type":"markdown","source":"                                              Ravdess Dataframe\nHere is the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nSo, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4 This means the meta data for the audio file is:\n\n* Video-only (02)\n* Speech (01)\n* Fearful (06)\n* Normal intensity (01)\n* Statement \"dogs\" (02)\n* 1st Repetition (01)\n* 12th Actor (12) - Female (as the actor ID number is even)","metadata":{}},{"cell_type":"code","source":"#preparing data set\n\nravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nravdess_directory_list = os.listdir(ravdess)\nprint(ravdess_directory_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:33.728436Z","iopub.execute_input":"2025-05-11T14:08:33.728775Z","iopub.status.idle":"2025-05-11T14:08:33.740651Z","shell.execute_reply.started":"2025-05-11T14:08:33.728749Z","shell.execute_reply":"2025-05-11T14:08:33.739840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Crema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:37.497447Z","iopub.execute_input":"2025-05-11T14:08:37.497780Z","iopub.status.idle":"2025-05-11T14:08:37.501587Z","shell.execute_reply.started":"2025-05-11T14:08:37.497759Z","shell.execute_reply":"2025-05-11T14:08:37.500789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# preprocessing","metadata":{}},{"cell_type":"markdown","source":"**Ravdees**","metadata":{}},{"cell_type":"code","source":"file_emotion = []\nfile_path = []\nfor i in ravdess_directory_list:\n    # as their are 24 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(ravdess + i)\n    for f in actor:\n        part = f.split('.')[0].split('-')\n    # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(ravdess + i + '/' + f)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:40.997686Z","iopub.execute_input":"2025-05-11T14:08:40.998343Z","iopub.status.idle":"2025-05-11T14:08:41.325932Z","shell.execute_reply.started":"2025-05-11T14:08:40.998321Z","shell.execute_reply":"2025-05-11T14:08:41.325138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(actor[0])\nprint(part[0])\nprint(file_path[0])\nprint(int(part[2]))\nprint(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:44.436894Z","iopub.execute_input":"2025-05-11T14:08:44.437411Z","iopub.status.idle":"2025-05-11T14:08:44.442072Z","shell.execute_reply.started":"2025-05-11T14:08:44.437382Z","shell.execute_reply":"2025-05-11T14:08:44.441473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nravdess_df = pd.concat([emotion_df, path_df], axis=1)\n# changing integers to actual emotions.\nravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust',\n                             8:'surprise'},\n                            inplace=True)\nprint(ravdess_df.head())\nprint(\"______________________________________________\")\nprint(ravdess_df.tail())\nprint(\"_______________________________________________\")\nprint(ravdess_df.Emotions.value_counts())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:46.731175Z","iopub.execute_input":"2025-05-11T14:08:46.731893Z","iopub.status.idle":"2025-05-11T14:08:46.763278Z","shell.execute_reply.started":"2025-05-11T14:08:46.731868Z","shell.execute_reply":"2025-05-11T14:08:46.762534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Crema DataFrame**","metadata":{}},{"cell_type":"markdown","source":"CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).","metadata":{}},{"cell_type":"code","source":"crema_directory_list = os.listdir(Crema)\n\nfile_emotion = []\nfile_path = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_path.append(Crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        file_emotion.append('angry')\n    elif part[2] == 'DIS':\n        file_emotion.append('disgust')\n    elif part[2] == 'FEA':\n        file_emotion.append('fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_emotion.append('neutral')\n    else:\n        file_emotion.append('Unknown')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nCrema_df = pd.concat([emotion_df, path_df], axis=1)\nCrema_df.head()\nprint(Crema_df.Emotions.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:54.344877Z","iopub.execute_input":"2025-05-11T14:08:54.345576Z","iopub.status.idle":"2025-05-11T14:08:54.432209Z","shell.execute_reply.started":"2025-05-11T14:08:54.345551Z","shell.execute_reply":"2025-05-11T14:08:54.431379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TESS dataset**","metadata":{}},{"cell_type":"markdown","source":"There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.\n\nThe dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format","metadata":{}},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\nTess_df.head()\nprint(Tess_df.Emotions.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:08:57.738263Z","iopub.execute_input":"2025-05-11T14:08:57.738881Z","iopub.status.idle":"2025-05-11T14:08:57.929535Z","shell.execute_reply.started":"2025-05-11T14:08:57.738857Z","shell.execute_reply":"2025-05-11T14:08:57.928944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**SAVEE Dataset**","metadata":{}},{"cell_type":"markdown","source":"Context\nThe SAVEE database was recorded from four native English male speakers (identified as DC, JE, JK, KL), postgraduate students and researchers at the University of Surrey aged from 27 to 31 years. Emotion has been described psychologically in discrete categories: anger, disgust, fear, happiness, sadness and surprise. This is supported by the cross-cultural studies of Ekman [6] and studies of automatic emotion recognition tended to focus on recognizing these [12]. We added neutral to provide recordings of 7 emotion categories. The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phonetically-balanced. The 3 common and 2 Ã— 6 = 12 emotion-specific sentences were recorded as neutral to give 30 neutral sentences.\n\nContent\nThis results in a total of 120 utterances per speaker, for example:\n\nCommon: She had your dark suit in greasy wash water all year.\nAnger: Who authorized the unlimited expense account?\nDisgust: Please take this dirty table cloth to the cleaners for me.\nFear: Call an ambulance for medical assistance.\nHappiness: Those musicians harmonize marvelously.\nSadness: The prospect of cutting back spending is an unpleasant one for any governor.\nSurprise: The carpet cleaners shampooed our oriental rug.\nNeutral: The best way to learn is to solve extra problems.","metadata":{}},{"cell_type":"code","source":"savee_directory_list = os.listdir(Savee)\n\nfile_emotion = []\nfile_path = []\n\nfor file in savee_directory_list:\n    file_path.append(Savee + file)\n    part = file.split('_')[1]\n    ele = part[:-6]\n    if ele=='a':\n        file_emotion.append('angry')\n    elif ele=='d':\n        file_emotion.append('disgust')\n    elif ele=='f':\n        file_emotion.append('fear')\n    elif ele=='h':\n        file_emotion.append('happy')\n    elif ele=='n':\n        file_emotion.append('neutral')\n    elif ele=='sa':\n        file_emotion.append('sad')\n    else:\n        file_emotion.append('surprise')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nSavee_df = pd.concat([emotion_df, path_df], axis=1)\nSavee_df.head()\nprint(Savee_df.Emotions.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:06.417659Z","iopub.execute_input":"2025-05-11T14:09:06.418233Z","iopub.status.idle":"2025-05-11T14:09:06.448933Z","shell.execute_reply.started":"2025-05-11T14:09:06.418213Z","shell.execute_reply":"2025-05-11T14:09:06.448204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Integration**","metadata":{}},{"cell_type":"code","source":"# creating Dataframe using all the 4 dataframes we created so far.\ndata_path = pd.concat([ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:09.825809Z","iopub.execute_input":"2025-05-11T14:09:09.826082Z","iopub.status.idle":"2025-05-11T14:09:09.882692Z","shell.execute_reply.started":"2025-05-11T14:09:09.826064Z","shell.execute_reply":"2025-05-11T14:09:09.881973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data_path.Emotions.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:14.098307Z","iopub.execute_input":"2025-05-11T14:09:14.098817Z","iopub.status.idle":"2025-05-11T14:09:14.104833Z","shell.execute_reply.started":"2025-05-11T14:09:14.098793Z","shell.execute_reply":"2025-05-11T14:09:14.103920Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":">*                           Data Visualisation and Exploration","metadata":{}},{"cell_type":"code","source":"data,sr = librosa.load(file_path[0])\nsr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:16.833561Z","iopub.execute_input":"2025-05-11T14:09:16.833837Z","iopub.status.idle":"2025-05-11T14:09:28.259328Z","shell.execute_reply.started":"2025-05-11T14:09:16.833819Z","shell.execute_reply":"2025-05-11T14:09:28.258567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ipd.Audio(data,rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:32.045729Z","iopub.execute_input":"2025-05-11T14:09:32.046693Z","iopub.status.idle":"2025-05-11T14:09:32.074870Z","shell.execute_reply.started":"2025-05-11T14:09:32.046668Z","shell.execute_reply":"2025-05-11T14:09:32.074243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nlog_spectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(log_spectrogram, y_axis='mel', sr=sr, x_axis='time');\nplt.title('Mel Spectrogram ')\nplt.colorbar(format='%+2.0f dB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:37.482379Z","iopub.execute_input":"2025-05-11T14:09:37.482978Z","iopub.status.idle":"2025-05-11T14:09:39.008043Z","shell.execute_reply.started":"2025-05-11T14:09:37.482954Z","shell.execute_reply":"2025-05-11T14:09:39.007372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(data,rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:42.324307Z","iopub.execute_input":"2025-05-11T14:09:42.324938Z","iopub.status.idle":"2025-05-11T14:09:42.532805Z","shell.execute_reply.started":"2025-05-11T14:09:42.324912Z","shell.execute_reply":"2025-05-11T14:09:42.532054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data augmentation","metadata":{}},{"cell_type":"code","source":"# NOISE\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\n# STRETCH\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(y=data, rate=rate)\n# SHIFT\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n# PITCH\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:46.042087Z","iopub.execute_input":"2025-05-11T14:09:46.042364Z","iopub.status.idle":"2025-05-11T14:09:46.047437Z","shell.execute_reply.started":"2025-05-11T14:09:46.042345Z","shell.execute_reply":"2025-05-11T14:09:46.046794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NORMAL AUDIO\n\n\nimport librosa.display\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=data, sr=sr)\nipd.Audio(data,rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:48.553419Z","iopub.execute_input":"2025-05-11T14:09:48.554165Z","iopub.status.idle":"2025-05-11T14:09:49.023592Z","shell.execute_reply.started":"2025-05-11T14:09:48.554140Z","shell.execute_reply":"2025-05-11T14:09:49.022637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# AUDIO WITH NOISE\nx = noise(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:52.305150Z","iopub.execute_input":"2025-05-11T14:09:52.305426Z","iopub.status.idle":"2025-05-11T14:09:52.807020Z","shell.execute_reply.started":"2025-05-11T14:09:52.305407Z","shell.execute_reply":"2025-05-11T14:09:52.806332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STRETCHED AUDIO\nx = stretch(data)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:09:55.793615Z","iopub.execute_input":"2025-05-11T14:09:55.794311Z","iopub.status.idle":"2025-05-11T14:09:57.051018Z","shell.execute_reply.started":"2025-05-11T14:09:55.794288Z","shell.execute_reply":"2025-05-11T14:09:57.050239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SHIFTED AUDIO\nx = shift(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:10:00.212193Z","iopub.execute_input":"2025-05-11T14:10:00.212799Z","iopub.status.idle":"2025-05-11T14:10:00.665786Z","shell.execute_reply.started":"2025-05-11T14:10:00.212777Z","shell.execute_reply":"2025-05-11T14:10:00.665017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# AUDIO WITH PITCH\nx = pitch(data, sr)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=x, sr=sr)\nipd.Audio(x, rate=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:10:03.168804Z","iopub.execute_input":"2025-05-11T14:10:03.169112Z","iopub.status.idle":"2025-05-11T14:10:03.632362Z","shell.execute_reply.started":"2025-05-11T14:10:03.169092Z","shell.execute_reply":"2025-05-11T14:10:03.631712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature extraction","metadata":{}},{"cell_type":"code","source":"def zcr(data,frame_length,hop_length):\n    zcr=librosa.feature.zero_crossing_rate(y=data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(zcr)\ndef rmse(data,frame_length=2048,hop_length=512):\n    rmse=librosa.feature.rms(y=data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(rmse)\ndef mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n    mfcc=librosa.feature.mfcc(y=data,sr=sr)\n    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n\ndef extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n    result=np.array([])\n    \n    result=np.hstack((result,\n                      zcr(data,frame_length,hop_length),\n                      rmse(data,frame_length,hop_length),\n                      mfcc(data,sr,frame_length,hop_length)\n                     ))\n    return result\n\ndef get_features(path,duration=2.5, offset=0.6):\n    data,sr=librosa.load(path,duration=duration,offset=offset)\n    aud=extract_features(data)\n    audio=np.array(aud)\n    \n    noised_audio=noise(data)\n    aud2=extract_features(noised_audio)\n    audio=np.vstack((audio,aud2))\n    \n    pitched_audio=pitch(data,sr)\n    aud3=extract_features(pitched_audio)\n    audio=np.vstack((audio,aud3))\n    \n    pitched_audio1=pitch(data,sr)\n    pitched_noised_audio=noise(pitched_audio1)\n    aud4=extract_features(pitched_noised_audio)\n    audio=np.vstack((audio,aud4))\n    \n    return audio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:10:14.003470Z","iopub.execute_input":"2025-05-11T14:10:14.004068Z","iopub.status.idle":"2025-05-11T14:10:14.011493Z","shell.execute_reply.started":"2025-05-11T14:10:14.004042Z","shell.execute_reply":"2025-05-11T14:10:14.010715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import multiprocessing as mp\nprint(\"Number of processors: \", mp.cpu_count())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:10:18.097907Z","iopub.execute_input":"2025-05-11T14:10:18.098713Z","iopub.status.idle":"2025-05-11T14:10:18.102653Z","shell.execute_reply.started":"2025-05-11T14:10:18.098678Z","shell.execute_reply":"2025-05-11T14:10:18.102050Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Noraml way to get features","metadata":{}},{"cell_type":"code","source":"# import timeit\n# from tqdm import tqdm\n# start = timeit.default_timer()\n# X,Y=[],[]\n# for path,emotion,index in tqdm (zip(data_path.Path,data_path.Emotions,range(data_path.Path.shape[0]))):\n#     features=get_features(path)\n#     if index%500==0:\n#         print(f'{index} audio has been processed')\n#     for i in features:\n#         X.append(i)\n#         Y.append(emotion)\n# print('Done')\n# stop = timeit.default_timer()\n\n# print('Time: ', stop - start)         ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:50.337238Z","iopub.execute_input":"2025-01-26T11:47:50.337515Z","iopub.status.idle":"2025-01-26T11:47:54.0592Z","shell.execute_reply.started":"2025-01-26T11:47:50.337492Z","shell.execute_reply":"2025-01-26T11:47:54.055429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Faster way to get features\n***Parallel way***\n\n**Dont be afraid from red lines that Normal**\n\n\nThis code is an example of how to use the joblib library to process multiple audio files in parallel using the process_feature function. The code also uses the timeit library to measure the time taken to process the audio files.\n\nHere's a breakdown of what the code does:\n\nThe from joblib import Parallel, delayed statement imports the Parallel and delayed functions from the joblib library.\nThe start = timeit.default_timer() statement starts a timer to measure the time taken to process the audio files.\nThe process_feature function processes a single audio file by extracting its features using the get_feat function and appending the corresponding X and Y values to the X and Y lists.\nThe paths and emotions variables extract the paths and emotions from the data_path DataFrame.\nThe Parallel function runs the process_feature function in parallel for each audio file using the delayed function to wrap the process_feature function.\nThe results variable contains the X and Y values for each audio file.\nThe X and Y lists are populated with the X and Y values from each audio file using the extend method.\nThe stop = timeit.default_timer() statement stops the timer.\nThe print('Time: ', stop - start) statement prints the time taken to process the audio files.\nOverall, this code demonstrates how to use the joblib library to process multiple audio files in parallel, which can significantly reduce the processing time for large datasets.This code is an example of how to use the joblib library to process multiple audio files in parallel using the process_feature function. The code also uses the timeit library to measure the time taken to process the audio files.\n\nHere's a breakdown of what the code does:\n\nThe from joblib import Parallel, delayed statement imports the Parallel and delayed functions from the joblib library.\nThe start = timeit.default_timer() statement starts a timer to measure the time taken to process the audio files.\nThe process_feature function processes a single audio file by extracting its features using the get_feat function and appending the corresponding X and Y values to the X and Y lists.\nThe paths and emotions variables extract the paths and emotions from the data_path DataFrame.\nThe Parallel function runs the process_feature function in parallel for each audio file using the delayed function to wrap the process_feature function.\nThe results variable contains the X and Y values for each audio file.\nThe X and Y lists are populated with the X and Y values from each audio file using the extend method.\nThe stop = timeit.default_timer() statement stops the timer.\nThe print('Time: ', stop - start) statement prints the time taken to process the audio files.\nOverall, this code demonstrates how to use the joblib library to process multiple audio files in parallel, which can significantly reduce the processing time for large datasets.","metadata":{}},{"cell_type":"markdown","source":"*  The .extend() method increases the length of the list by the number of elements that are provided to the method, so if you want to add multiple elements to the list, you can use this method.","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\nimport timeit\nstart = timeit.default_timer()\n# Define a function to get features for a single audio file\ndef process_feature(path, emotion):\n    features = get_features(path)\n    X = []\n    Y = []\n    for ele in features:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\n    print(X,Y)\n    return X, Y\n\npaths = data_path.Path\nemotions = data_path.Emotions\n\n# Run the loop in parallel\nresults = Parallel(n_jobs=-1)(delayed(process_feature)(path, emotion) for (path, emotion) in zip(paths, emotions))\n\n# Collect the results\nX = []\nY = []\nfor result in results:\n    x, y = result\n    X.extend(x)\n    Y.extend(y)\n\n\nstop = timeit.default_timer()\n\nprint('Time: ', stop - start)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-05-11T14:10:22.815772Z","iopub.execute_input":"2025-05-11T14:10:22.816320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# stop-start","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:13:30.061462Z","iopub.status.idle":"2025-02-17T07:13:30.061729Z","shell.execute_reply":"2025-02-17T07:13:30.061625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(X), len(Y), data_path.Path.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T07:13:30.062631Z","iopub.status.idle":"2025-02-17T07:13:30.063091Z","shell.execute_reply":"2025-02-17T07:13:30.062899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving features","metadata":{}},{"cell_type":"code","source":"Emotions = pd.DataFrame(X)\nEmotions['Emotions'] = Y\nEmotions.to_csv('emotion.csv', index=False)\nEmotions.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:44:12.096866Z","iopub.execute_input":"2025-05-11T14:44:12.097483Z","iopub.status.idle":"2025-05-11T14:47:04.363567Z","shell.execute_reply.started":"2025-05-11T14:44:12.097450Z","shell.execute_reply":"2025-05-11T14:47:04.362936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Emotions = pd.read_csv('/kaggle/input/zcr-rmse-mfcc-dataset-all-augmented/emotion.csv')\nEmotions.head()","metadata":{"execution":{"iopub.status.busy":"2025-05-11T12:16:07.879519Z","iopub.execute_input":"2025-05-11T12:16:07.879860Z","iopub.status.idle":"2025-05-11T12:16:07.903474Z","shell.execute_reply.started":"2025-05-11T12:16:07.879834Z","shell.execute_reply":"2025-05-11T12:16:07.902278Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(Emotions.isna().any())\n","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:47:30.463396Z","iopub.execute_input":"2025-05-11T14:47:30.463932Z","iopub.status.idle":"2025-05-11T14:47:30.729852Z","shell.execute_reply.started":"2025-05-11T14:47:30.463909Z","shell.execute_reply":"2025-05-11T14:47:30.729274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Emotions=Emotions.fillna(0)\nprint(Emotions.isna().any())\nEmotions.shape","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:47:36.626477Z","iopub.execute_input":"2025-05-11T14:47:36.627249Z","iopub.status.idle":"2025-05-11T14:47:37.514139Z","shell.execute_reply.started":"2025-05-11T14:47:36.627224Z","shell.execute_reply":"2025-05-11T14:47:37.513553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.sum(Emotions.isna())","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:47:44.104023Z","iopub.execute_input":"2025-05-11T14:47:44.104576Z","iopub.status.idle":"2025-05-11T14:47:44.434197Z","shell.execute_reply.started":"2025-05-11T14:47:44.104555Z","shell.execute_reply":"2025-05-11T14:47:44.433567Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"#taking all rows and all cols without last col for X which include features\n#taking last col for Y, which include the emotions\n\n\nX = Emotions.iloc[: ,:-1].values\nY = Emotions['Emotions'].values","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:47:48.064057Z","iopub.execute_input":"2025-05-11T14:47:48.064478Z","iopub.status.idle":"2025-05-11T14:47:48.334065Z","shell.execute_reply.started":"2025-05-11T14:47:48.064457Z","shell.execute_reply":"2025-05-11T14:47:48.333454Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:47:54.854163Z","iopub.execute_input":"2025-05-11T14:47:54.854671Z","iopub.status.idle":"2025-05-11T14:47:54.876168Z","shell.execute_reply.started":"2025-05-11T14:47:54.854650Z","shell.execute_reply":"2025-05-11T14:47:54.875537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(Y.shape)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:47:58.367450Z","iopub.execute_input":"2025-05-11T14:47:58.367955Z","iopub.status.idle":"2025-05-11T14:47:58.373143Z","shell.execute_reply.started":"2025-05-11T14:47:58.367932Z","shell.execute_reply":"2025-05-11T14:47:58.372387Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42,test_size=0.2, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:01.283753Z","iopub.execute_input":"2025-05-11T14:48:01.284302Z","iopub.status.idle":"2025-05-11T14:48:02.579851Z","shell.execute_reply.started":"2025-05-11T14:48:01.284283Z","shell.execute_reply":"2025-05-11T14:48:02.579094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#reshape for lstm\nX_train = x_train.reshape(x_train.shape[0] , x_train.shape[1] , 1)\nX_test = x_test.reshape(x_test.shape[0] , x_test.shape[1] , 1)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:07.445787Z","iopub.execute_input":"2025-05-11T14:48:07.446458Z","iopub.status.idle":"2025-05-11T14:48:07.450085Z","shell.execute_reply.started":"2025-05-11T14:48:07.446435Z","shell.execute_reply":"2025-05-11T14:48:07.449410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:11.681203Z","iopub.execute_input":"2025-05-11T14:48:11.681941Z","iopub.status.idle":"2025-05-11T14:48:13.459130Z","shell.execute_reply.started":"2025-05-11T14:48:11.681908Z","shell.execute_reply":"2025-05-11T14:48:13.458551Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization , GRU\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:16.511860Z","iopub.execute_input":"2025-05-11T14:48:16.512597Z","iopub.status.idle":"2025-05-11T14:48:16.516869Z","shell.execute_reply.started":"2025-05-11T14:48:16.512572Z","shell.execute_reply":"2025-05-11T14:48:16.516220Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Applying early stopping for all models\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nmodel_checkpoint = ModelCheckpoint('best_model1_weights.keras', monitor='val_accuracy', save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:20.908552Z","iopub.execute_input":"2025-05-11T14:48:20.909243Z","iopub.status.idle":"2025-05-11T14:48:20.915159Z","shell.execute_reply.started":"2025-05-11T14:48:20.909219Z","shell.execute_reply":"2025-05-11T14:48:20.914470Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nearly_stop=EarlyStopping(monitor='val_acc',mode='max',patience=5,restore_best_weights=True)\nlr_reduction=ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.5,min_lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:25.295346Z","iopub.execute_input":"2025-05-11T14:48:25.295896Z","iopub.status.idle":"2025-05-11T14:48:25.299552Z","shell.execute_reply.started":"2025-05-11T14:48:25.295872Z","shell.execute_reply":"2025-05-11T14:48:25.298906Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"markdown","source":"Model that have lstm layers take alot of time if you have much free time enjoy with it","metadata":{}},{"cell_type":"code","source":"\"\"\"model01=Sequential()\nmodel01.add(LSTM(128,return_sequences=True,input_shape=(x_train.shape[1],1)))\nmodel01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.2))\nmodel01.add(LSTM(128,return_sequences=True))\n#model01.add(Dropout(0.3))\nmodel01.add(LSTM(128))\n#model01.add(Dropout(0.3))\nmodel01.add(Dense(7,activation = 'softmax'))\nmodel01.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel01.summary()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-01-26T11:47:54.089493Z","iopub.status.idle":"2025-01-26T11:47:54.089949Z","shell.execute_reply.started":"2025-01-26T11:47:54.089704Z","shell.execute_reply":"2025-01-26T11:47:54.089726Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"hist=model01.fit(X_train, y_train,\n            epochs=20,\n            validation_data=(X_test, y_test),batch_size=64,\n            verbose=1)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-01-26T11:47:54.091286Z","iopub.status.idle":"2025-01-26T11:47:54.091721Z","shell.execute_reply.started":"2025-01-26T11:47:54.091492Z","shell.execute_reply":"2025-01-26T11:47:54.091512Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"print(\"Accuracy of our model on test data : \" , model01.evaluate(X_test,y_test)[1]*100 , \"%\")\nepochs = [i for i in range(20)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = hist.history['accuracy']\ntrain_loss = hist.history['loss']\ntest_acc = hist.history['val_accuracy']\ntest_loss = hist.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:54.092689Z","iopub.status.idle":"2025-01-26T11:47:54.09312Z","shell.execute_reply.started":"2025-01-26T11:47:54.092892Z","shell.execute_reply":"2025-01-26T11:47:54.092913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN model","metadata":{}},{"cell_type":"code","source":"#Reshape for CNN_LSTM MODEL\n\nx_traincnn =np.expand_dims(x_train, axis=2)\nx_testcnn= np.expand_dims(x_test, axis=2)\nx_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape\n#x_testcnn[0]","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:38.764076Z","iopub.execute_input":"2025-05-11T14:48:38.764348Z","iopub.status.idle":"2025-05-11T14:48:38.770180Z","shell.execute_reply.started":"2025-05-11T14:48:38.764329Z","shell.execute_reply":"2025-05-11T14:48:38.769455Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow.keras.layers as L\n\nmodel = tf.keras.Sequential([\n    L.Conv1D(512,kernel_size=5, strides=1,padding='same', activation='relu',input_shape=(X_train.shape[1],1)),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(512,kernel_size=5,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the second max pooling layer\n    \n    L.Conv1D(256,kernel_size=5,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    \n    L.Conv1D(256,kernel_size=3,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=5,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fourth max pooling layer\n    \n    L.Conv1D(128,kernel_size=3,strides=1,padding='same',activation='relu'),\n    L.BatchNormalization(),\n    L.MaxPool1D(pool_size=3,strides=2,padding='same'),\n    Dropout(0.2),  # Add dropout layer after the fifth max pooling layer\n    \n    L.Flatten(),\n    L.Dense(512,activation='relu'),\n    L.BatchNormalization(),\n    L.Dense(7,activation='softmax')\n])\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:42.863271Z","iopub.execute_input":"2025-05-11T14:48:42.863571Z","iopub.status.idle":"2025-05-11T14:48:45.237203Z","shell.execute_reply.started":"2025-05-11T14:48:42.863551Z","shell.execute_reply":"2025-05-11T14:48:45.236683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history=model.fit(x_traincnn, y_train, epochs=50, validation_data=(x_testcnn, y_test), batch_size=64,callbacks=[early_stop,lr_reduction,model_checkpoint])","metadata":{"execution":{"iopub.status.busy":"2025-05-11T14:48:59.384106Z","iopub.execute_input":"2025-05-11T14:48:59.384636Z","iopub.status.idle":"2025-05-11T15:57:45.796271Z","shell.execute_reply.started":"2025-05-11T14:48:59.384612Z","shell.execute_reply":"2025-05-11T15:57:45.795553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:02:35.093016Z","iopub.execute_input":"2025-05-11T16:02:35.093641Z","iopub.status.idle":"2025-05-11T16:02:41.340360Z","shell.execute_reply.started":"2025-05-11T16:02:35.093616Z","shell.execute_reply":"2025-05-11T16:02:41.339646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predicting on test data.\npred_test0 = model.predict(x_testcnn)\ny_pred0 = encoder.inverse_transform(pred_test0)\ny_test0 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred0.flatten()\ndf0['Actual Labels'] = y_test0.flatten()\n\ndf0.head(10)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:03:35.745888Z","iopub.execute_input":"2025-05-11T16:03:35.746472Z","iopub.status.idle":"2025-05-11T16:03:41.479851Z","shell.execute_reply.started":"2025-05-11T16:03:35.746453Z","shell.execute_reply":"2025-05-11T16:03:41.479020Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df0","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:03:49.224349Z","iopub.execute_input":"2025-05-11T16:03:49.224869Z","iopub.status.idle":"2025-05-11T16:03:49.232791Z","shell.execute_reply.started":"2025-05-11T16:03:49.224839Z","shell.execute_reply":"2025-05-11T16:03:49.232201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Some plots of multi_model\n______________________________________________\n","metadata":{}},{"cell_type":"markdown","source":"# CLSTM Model","metadata":{}},{"cell_type":"markdown","source":"Model that have lstm layers take alot of time if you have much free time enjoy with it","metadata":{}},{"cell_type":"markdown","source":"Another  model (CLSTM)  omnia model\n_____________________________________________________","metadata":{}},{"cell_type":"code","source":"#Build the model\n\n# define model\n\"\"\"model000 = Sequential()\nmodel000.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X.shape[1], 1)))\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel000.add(BatchNormalization())\nmodel000.add(Dropout(0.3))\n\n          \nmodel000.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel000.add(BatchNormalization())\nmodel000.add(Dropout(0.3))\n\nmodel000.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel000.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel000.add(BatchNormalization())\nmodel000.add(Dropout(0.3))\n          \nmodel000.add(LSTM(128, return_sequences=True)) \nmodel000.add(Dropout(0.3))\n\nmodel000.add(LSTM(128, return_sequences=True)) \nmodel000.add(Dropout(0.3))\nmodel000.add(LSTM(128))\nmodel000.add(Dropout(0.3))\n\nmodel000.add(Dense(128, activation='relu'))\n#model000.add(Dropout(0.3))\n\nmodel000.add(Dense(64, activation='relu'))\n#model000.add(Dropout(0.3))\n\nmodel000.add(Dense(32, activation='relu'))\n#model000.add(Dropout(0.3))\n\nmodel000.add(Dense(7, activation='softmax'))\n\n\n\nmodel000.summary()\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:54.105038Z","iopub.status.idle":"2025-01-26T11:47:54.105352Z","shell.execute_reply.started":"2025-01-26T11:47:54.105206Z","shell.execute_reply":"2025-01-26T11:47:54.105221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"from keras.utils.vis_utils import plot_model\nplot_model( model000, show_shapes=True, show_layer_names=True, to_file='model000.png')\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:54.106871Z","iopub.status.idle":"2025-01-26T11:47:54.107203Z","shell.execute_reply.started":"2025-01-26T11:47:54.107022Z","shell.execute_reply":"2025-01-26T11:47:54.107037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"model000.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:54.108051Z","iopub.status.idle":"2025-01-26T11:47:54.108392Z","shell.execute_reply.started":"2025-01-26T11:47:54.108242Z","shell.execute_reply":"2025-01-26T11:47:54.108257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"hist1=model000.fit(x_traincnn, y_train, batch_size=64, epochs=40, validation_data=(x_testcnn, y_test))\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:54.109442Z","iopub.status.idle":"2025-01-26T11:47:54.109741Z","shell.execute_reply.started":"2025-01-26T11:47:54.109597Z","shell.execute_reply":"2025-01-26T11:47:54.109611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"print(\"Accuracy of our model on test data : \" , model000.evaluate(x_testcnn,y_test)[1]*100 , \"%\")\nepochs = [i for i in range(40)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = hist1.history['accuracy']\ntrain_loss = hist1.history['loss']\ntest_acc = hist1.history['val_accuracy']\ntest_loss = hist1.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:54.111408Z","iopub.status.idle":"2025-01-26T11:47:54.111716Z","shell.execute_reply.started":"2025-01-26T11:47:54.111568Z","shell.execute_reply":"2025-01-26T11:47:54.111583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predicting on test data.\n\"\"\"pred_test00 = model000.predict(x_testcnn)\ny_pred00 = encoder.inverse_transform(pred_test)\ny_test00 = encoder.inverse_transform(y_test)\n\n# Check for random predictions\ndf0 = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf0['Predicted Labels'] = y_pred00.flatten()\ndf0['Actual Labels'] = y_test00.flatten()\n\ndf0.head(10)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:47:54.113439Z","iopub.status.idle":"2025-01-26T11:47:54.113713Z","shell.execute_reply.started":"2025-01-26T11:47:54.113577Z","shell.execute_reply":"2025-01-26T11:47:54.11359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evalutation","metadata":{}},{"cell_type":"markdown","source":"Results of best model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\ncm = confusion_matrix(y_test0, y_pred0)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='.2f')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()\nprint(classification_report(y_test0, y_pred0))","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:03:58.816492Z","iopub.execute_input":"2025-05-11T16:03:58.817178Z","iopub.status.idle":"2025-05-11T16:03:59.405777Z","shell.execute_reply.started":"2025-05-11T16:03:58.817155Z","shell.execute_reply":"2025-05-11T16:03:59.405027Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving Best Model","metadata":{}},{"cell_type":"code","source":"# MLP for Pima Indians Dataset Serialize to JSON and HDF5\nfrom tensorflow.keras.models import Sequential, model_from_json\nmodel_json = model.to_json()\nwith open(\"CNN_model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"CNN_model_.weights.h5\")\nprint(\"Saved model to disk\") ","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:11:50.319131Z","iopub.execute_input":"2025-05-11T16:11:50.319388Z","iopub.status.idle":"2025-05-11T16:11:50.559751Z","shell.execute_reply.started":"2025-05-11T16:11:50.319369Z","shell.execute_reply":"2025-05-11T16:11:50.558961Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loaded_model.load_weights(\"/kaggle/working/best_model1_weights.h5\")\nloaded_model.load_weights(\"/kaggle/working/CNN_model_.weights.h5\")\n\nprint(\"Loaded model from disk\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:47:38.441128Z","iopub.execute_input":"2025-05-11T17:47:38.441951Z","iopub.status.idle":"2025-05-11T17:47:38.590407Z","shell.execute_reply.started":"2025-05-11T17:47:38.441927Z","shell.execute_reply":"2025-05-11T17:47:38.589737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, model_from_json\njson_file = open('/kaggle/working/CNN_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\n#loaded_model.load_weights(\"/kaggle/working/best_model_.weights.h5\")\nloaded_model.load_weights(\"/kaggle/working/CNN_model_.weights.h5\")\nprint(\"Loaded model from disk\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:47:52.510120Z","iopub.execute_input":"2025-05-11T17:47:52.510388Z","iopub.status.idle":"2025-05-11T17:47:52.877625Z","shell.execute_reply.started":"2025-05-11T17:47:52.510371Z","shell.execute_reply":"2025-05-11T17:47:52.876808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nscore = loaded_model.evaluate(x_testcnn,y_test)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:14:03.346411Z","iopub.execute_input":"2025-05-11T16:14:03.347151Z","iopub.status.idle":"2025-05-11T16:14:09.409133Z","shell.execute_reply.started":"2025-05-11T16:14:03.347114Z","shell.execute_reply":"2025-05-11T16:14:09.408549Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving and Loading our Stnadrad Scaler and encoder\n* To save the StandardScaler object to use it later in a Flask API","metadata":{}},{"cell_type":"markdown","source":"pickle file\n","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Saving scaler\nwith open('scaler2.pickle', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Loading scaler\nwith open('scaler2.pickle', 'rb') as f:\n    scaler2 = pickle.load(f)\n\n# Saving encoder\nwith open('encoder2.pickle', 'wb') as f:\n    pickle.dump(encoder, f)\n\n# Loading encoder\nwith open('encoder2.pickle', 'rb') as f:\n    encoder2 = pickle.load(f)\n\n    \nprint(\"Done\")    ","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:14:24.009342Z","iopub.execute_input":"2025-05-11T16:14:24.009638Z","iopub.status.idle":"2025-05-11T16:14:24.016251Z","shell.execute_reply.started":"2025-05-11T16:14:24.009617Z","shell.execute_reply":"2025-05-11T16:14:24.015700Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test script\n* That can predict new record ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, model_from_json\njson_file = open('/kaggle/working/CNN_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\n#loaded_model.load_weights(\"/kaggle/working/best_model1_weights.h5\")\nloaded_model.load_weights(\"/kaggle/working/CNN_model_.weights.h5\")\n\nprint(\"Loaded model from disk\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:15:04.405381Z","iopub.execute_input":"2025-05-11T16:15:04.406161Z","iopub.status.idle":"2025-05-11T16:15:04.775115Z","shell.execute_reply.started":"2025-05-11T16:15:04.406135Z","shell.execute_reply":"2025-05-11T16:15:04.774491Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/working/scaler2.pickle', 'rb') as f:\n    scaler2 = pickle.load(f)\n    \nwith open('/kaggle/working/encoder2.pickle', 'rb') as f:\n    encoder2 = pickle.load(f)\n\n    \nprint(\"Done\")    ","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:15:21.502144Z","iopub.execute_input":"2025-05-11T16:15:21.502416Z","iopub.status.idle":"2025-05-11T16:15:21.507535Z","shell.execute_reply.started":"2025-05-11T16:15:21.502397Z","shell.execute_reply":"2025-05-11T16:15:21.506939Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa","metadata":{"execution":{"iopub.status.busy":"2025-05-11T16:15:25.023286Z","iopub.execute_input":"2025-05-11T16:15:25.023954Z","iopub.status.idle":"2025-05-11T16:15:25.026970Z","shell.execute_reply.started":"2025-05-11T16:15:25.023932Z","shell.execute_reply":"2025-05-11T16:15:25.026342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zcr(data,frame_length,hop_length):\n    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(zcr)\ndef rmse(data,frame_length=2048,hop_length=512):\n    rmse=librosa.feature.rms(y=data,frame_length=frame_length,hop_length=hop_length)\n    return np.squeeze(rmse)\ndef mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n    mfcc=librosa.feature.mfcc(y=data,sr=sr)\n    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n\ndef extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n    result=np.array([])\n    \n    result=np.hstack((result,\n                      zcr(data,frame_length,hop_length),\n                                  rmse(data,frame_length,hop_length),\n                                  mfcc(data,sr,frame_length,hop_length)\n\n                   ))\n    return result","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:15:27.000706Z","iopub.execute_input":"2025-05-11T17:15:27.000972Z","iopub.status.idle":"2025-05-11T17:15:27.007041Z","shell.execute_reply.started":"2025-05-11T17:15:27.000955Z","shell.execute_reply":"2025-05-11T17:15:27.006226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predict_feat(path):\n    d, s_rate= librosa.load(path, duration=2.5, offset=0.6)\n    res=extract_features(d)\n    result=np.array(res)\n    result=np.reshape(result,newshape=(1,2376))\n    i_result = scaler2.transform(result)\n    final_result=np.expand_dims(i_result, axis=2)\n    \n    return final_result","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:15:32.727934Z","iopub.execute_input":"2025-05-11T17:15:32.728398Z","iopub.status.idle":"2025-05-11T17:15:32.732702Z","shell.execute_reply.started":"2025-05-11T17:15:32.728377Z","shell.execute_reply":"2025-05-11T17:15:32.731828Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res=get_predict_feat(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-07-01-01-01-01.wav\")\nprint(res.shape)","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:15:36.241728Z","iopub.execute_input":"2025-05-11T17:15:36.242256Z","iopub.status.idle":"2025-05-11T17:15:36.276610Z","shell.execute_reply.started":"2025-05-11T17:15:36.242236Z","shell.execute_reply":"2025-05-11T17:15:36.276017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotions1={1:'Neutral', 2:'Calm', 3:'Happy', 4:'Sad', 5:'Angry', 6:'Fear', 7:'Disgust',8:'Surprise'}\ndef prediction(path1):\n    res=get_predict_feat(path1)\n    predictions=loaded_model.predict(res)\n    y_pred = encoder2.inverse_transform(predictions)\n    print(y_pred[0][0])    ","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:15:55.618236Z","iopub.execute_input":"2025-05-11T17:15:55.618687Z","iopub.status.idle":"2025-05-11T17:15:55.622630Z","shell.execute_reply.started":"2025-05-11T17:15:55.618668Z","shell.execute_reply":"2025-05-11T17:15:55.621924Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-01-01-01-01-02.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:16:02.963624Z","iopub.execute_input":"2025-05-11T17:16:02.963875Z","iopub.status.idle":"2025-05-11T17:16:03.944270Z","shell.execute_reply.started":"2025-05-11T17:16:02.963857Z","shell.execute_reply":"2025-05-11T17:16:03.943545Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:16:13.703465Z","iopub.execute_input":"2025-05-11T17:16:13.704020Z","iopub.status.idle":"2025-05-11T17:16:13.812281Z","shell.execute_reply.started":"2025-05-11T17:16:13.703996Z","shell.execute_reply":"2025-05-11T17:16:13.811254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-02-02-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:16:17.707775Z","iopub.execute_input":"2025-05-11T17:16:17.708435Z","iopub.status.idle":"2025-05-11T17:16:17.816876Z","shell.execute_reply.started":"2025-05-11T17:16:17.708414Z","shell.execute_reply":"2025-05-11T17:16:17.816295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_21/03-01-04-02-02-02-21.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:16:21.496061Z","iopub.execute_input":"2025-05-11T17:16:21.496622Z","iopub.status.idle":"2025-05-11T17:16:21.604858Z","shell.execute_reply.started":"2025-05-11T17:16:21.496600Z","shell.execute_reply":"2025-05-11T17:16:21.604285Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_02/03-01-06-01-02-02-02.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:16:27.635101Z","iopub.execute_input":"2025-05-11T17:16:27.635364Z","iopub.status.idle":"2025-05-11T17:16:27.739755Z","shell.execute_reply.started":"2025-05-11T17:16:27.635345Z","shell.execute_reply":"2025-05-11T17:16:27.739201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-08-01-01-01-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:16:31.255303Z","iopub.execute_input":"2025-05-11T17:16:31.255609Z","iopub.status.idle":"2025-05-11T17:16:31.357826Z","shell.execute_reply.started":"2025-05-11T17:16:31.255589Z","shell.execute_reply":"2025-05-11T17:16:31.357070Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-07-01-01-01-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2025-05-11T17:16:33.683002Z","iopub.execute_input":"2025-05-11T17:16:33.683706Z","iopub.status.idle":"2025-05-11T17:16:33.771359Z","shell.execute_reply.started":"2025-05-11T17:16:33.683681Z","shell.execute_reply":"2025-05-11T17:16:33.770826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}